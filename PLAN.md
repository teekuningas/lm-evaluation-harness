# FINBench-v2 Migration Plan

**Goal**: Migrate FINBench-v2 tasks from logprob-based (multiple_choice) to generate_until format to support thinking models (gpt-oss, deepseek-r1) that cannot provide token probabilities.

**Status**: Phase 1 & 2 Complete âœ… | Ready for Phase 3: Migration Script

---

## âœ… What We've Accomplished (2026-01-21)

### Phase 1: Fixed Existing Generate_Until Tasks

**Problem**: Squad and TruthfulQA had problematic `until` tokens truncating reasoning in thinking models.

**Files Modified**:
1. `lm_eval/tasks/finbench_v2/squad/gen/_squad_fi_yaml`
   - `until: ["\n"]` â†’ `until: []`
   - `max_new_tokens: 32` â†’ `max_gen_toks: 4096`
   - Removed `exact_match` metric (always 0% for chat models - verbose responses)
   
2. `lm_eval/tasks/finbench_v2/truthfulqa/gen/_ogx_truthfulqax_gen_fbv2_yaml`
   - Added explicit `generation_kwargs` block (was using problematic defaults)
   - `until: []` (was defaulting to `["\n\n"]`)
   - `max_gen_toks: 4096`

**Results** (gpt-oss-120b, LIMIT=3):
- TruthfulQA: **+175% BLEU**, **+71% ROUGE1** ðŸš€
- Squad: F1 stable at ~0.46 âœ…
- Zero null content errors âœ…
- Zero configuration warnings âœ…

### Phase 2: Created Schema-Aware Utils

**File Created**: `lm_eval/tasks/finbench_v2/utils.py` (542 lines)

**Components**:
- `get_choices_from_doc(doc)` - Handles 5 different v2 schemas (arc_c, belebele, goldenswag, truthfulqa, scandisent)
- `get_target_from_doc(doc, choices)` - Handles 6 different target formats (letter indices, 1-indexed, 0-indexed, words)
- `FINBenchV2AnswerFilter` - 6-layer answer extraction (substring, normalized, stripped, fuzzy, bold, stem) + extensive stats tracking

---

## ðŸŽ“ Key Learnings

### 1. **The `until` Token Problem**
- `until` parameter is passed as API `stop` sequences
- `until: ["\n\n"]` literally stops generation when `\n\n` appears
- This truncates reasoning in thinking models (happens mid-reasoning)
- **Solution**: `until: []` (empty - let model stop naturally via EOS tokens)

### 2. **Token Budget for Thinking Models**
- Thinking models need **5-10x more tokens** than non-thinking models
- Reasoning: 100-1000 tokens + Answer: 10-50 tokens
- **Solution**: `max_gen_toks: 4096` (was 32-2048)

### 3. **Squad Exact Match is Incompatible with Chat Models**
- SQuAD exact_match expects: `"Helsinki"` â†’ model outputs `"Helsinki"`
- Chat models generate: `"Helsinki"` â†’ `"Helsinki on Suomen pÃ¤Ã¤kaupunki"`
- Normalized strings differ â†’ always 0% exact match
- **Solution**: Use F1 score (token overlap) as primary metric, removed exact_match

### 4. **Schema Variations in v2 Tasks**
All 6 remaining v2 task categories are standard multiple-choice (good news!), but with different schemas:
- **arc_c**: `choices.text` list + `answerKey` letter ("A"-"D")
- **belebele**: Individual `mc_answer1-4` fields + `correct_answer_num` (1-indexed)
- **goldenswag**: Preprocessed `choices` list + `gold` integer (0-indexed)
- **scandisent**: Preprocessed `choices` list + `gold` word ("positiivinen"/"negatiivinen")
- **truthfulqa mc1/mc2**: `mc1_targets.choices` + always index 0
- **sib200**: Hardcoded 7 topics + `answer_idx` (0-indexed)

---

## ðŸ“‹ Next Steps: Phase 3 - Migration Script

### Task: Build `scripts/migrate_v2_to_generate_until.py`

**Input**: Existing finbench-v2 multiple_choice task YAML  
**Output**: New generate_until task YAML + updated group files

**Features**:
1. **Dry-run mode** (`--dry-run`) - Show what would be created without writing files
2. **Single task or category migration** - Flexible operation
3. **Template-based generation** - Use validated configs from Phase 1
4. **Safety checks**:
   - Don't overwrite existing generate_until tasks
   - Validate output YAML syntax
   - Backup group files before modification
5. **Extensive logging** - Track what's created, validation results

**Required Template Settings**:
```yaml
output_type: generate_until
generation_kwargs:
  until: []              # Critical: No stop sequences for thinking models
  max_gen_toks: 4096     # Sufficient for reasoning + answer
  do_sample: false
  temperature: 0
filter_list:
  - name: "finbench_v2_answer_extraction"
    filter:
      - function: !function utils.FINBenchV2AnswerFilter
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
    ignore_punctuation: true
```

**Usage Examples**:
```bash
# Dry-run single task
python scripts/migrate_v2_to_generate_until.py \
  --source lm_eval/tasks/finbench_v2/arc_c/cf/arc_challenge_fi_cf_fbv2_p0.yaml \
  --dry-run

# Migrate entire category (recommended)
python scripts/migrate_v2_to_generate_until.py \
  --category arc_c \
  --dry-run

# Actually migrate after reviewing dry-run
python scripts/migrate_v2_to_generate_until.py \
  --category arc_c

# Test migrated tasks
BENCHMARK=finbench_v2 SUBSET=arc_challenge_fi_gen LIMIT=5 bash run_benchmark.sh
```

### Migration Priority

**Total**: ~50 tasks to migrate across 6 categories

1. **arc_c** (10 tasks: 5 CF + 5 MCF)
   - Identical schema to finbench-v1 â†’ perfect test case
   - Expected: 0% invalid format if filter works correctly

2. **truthfulqa mc1** (5 tasks)
   - Standard single-correct MCQ
   - Already has gen version, add MC gen version

3. **goldenswag** (10 tasks: 5 CF + 5 MCF)
   - Already preprocessed to standard schema

4. **scandisent** (10 tasks: 5 CF + 5 MCF)
   - Binary classification formatted as MCQ

5. **belebele** (10 tasks: 5 CF + 5 MCF)
   - Different schema but utils.py handles it

6. **sib200** (5 tasks: MCF only)
   - 7 hardcoded topic choices

**Optional**: truthfulqa mc2 (5 tasks) - Multiple correct answers, complex evaluation

### Validation Checklist per Category

After migrating each category:
- [ ] Dry-run output looks correct
- [ ] YAML syntax valid (parse check)
- [ ] Test with LIMIT=5 to verify:
  - [ ] No null content errors
  - [ ] No configuration warnings
  - [ ] Filter stats show 0% invalid format rate
  - [ ] Exact match > 0% (proves extraction works)
  - [ ] Review logs for `âœ—âœ— INVALID FORMAT` errors

---

## ðŸ“– Documentation

**This file (`PLAN.md`)** contains all essential information:
- Current status (Phase 1 & 2 complete)
- Key learnings (4 critical discoveries)
- Next steps (build migration script)
- Testing strategy
- Success criteria

All analysis is consolidated inline above - no external docs needed.

---

## ðŸ”§ Testing Strategy

**Always test incrementally**:
1. Start with LIMIT=5 for new tasks
2. Check logs for errors/warnings
3. Review filter stats (target: <5% invalid format)
4. Compare metrics with logprob versions (sanity check)
5. If good, test with LIMIT=20
6. If still good, run full evaluation

**Key Log Checks**:
```bash
# Should be 0 results
grep "null content" results/*.log
grep "default.*generation_kwargs" results/*.log

# Should show our configs
grep "until: \[\]" results/*.log
grep "max_gen_toks: 4096" results/*.log

# Check filter performance
grep "FINBenchV2AnswerFilter Summary" results/*.log
```

---

## ðŸŽ¯ Success Criteria

### For Each Migrated Task Category:
- âœ… 0% null content errors
- âœ… <5% invalid format rate (FINBenchV2AnswerFilter)
- âœ… Exact match > 0% (proves extraction works)
- âœ… Metrics comparable to logprob versions (sanity check)

### Overall Migration Success:
- âœ… ~50 new generate_until tasks created
- âœ… All thinking model compatible (gpt-oss, deepseek-r1)
- âœ… Clean logs (no warnings/errors)
- âœ… Documented and tested

---

## ðŸ’¡ Notes for Next Session

### Quick Start
```bash
# Check current status
git status
cat PLAN.md

# See what's been done
ls lm_eval/tasks/finbench_v2/utils.py
git diff lm_eval/tasks/finbench_v2/squad/gen/_squad_fi_yaml
```

### Context to Remember
1. **`until: []` is critical** - Proven with +175% TruthfulQA improvement
2. **utils.py is ready** - Schema-aware extraction for 6 task types
3. **Squad exact_match removed** - Chat models are naturally verbose
4. **Ready for Phase 3** - Build migration script next

### Key Implementation Files
- `PLAN.md` (this file) - Complete guide
- `lm_eval/tasks/finbench_v2/utils.py` - Schema detection + filter
- `lm_eval/tasks/finbench_v2/squad/gen/_squad_fi_yaml` - Example fixed config
- `lm_eval/tasks/finbench_v2/truthfulqa/gen/_ogx_truthfulqax_gen_fbv2_yaml` - Example fixed config

---

**Last Updated**: 2026-01-21  
**Next Task**: Build migration script (`scripts/migrate_v2_to_generate_until.py`)
